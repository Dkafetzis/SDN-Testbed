{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f96b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "import keras\n",
    "import tensorflow\n",
    "import tensorflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e9a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b355efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcd36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88dd3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging.config\n",
    "import math\n",
    "import pkg_resources\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb275657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cfg_load in /usr/local/lib/python3.9/dist-packages (0.9.0)\r\n",
      "Requirement already satisfied: PyYAML>=4.2b1 in /usr/lib/python3/dist-packages (from cfg_load) (5.3.1)\r\n",
      "Requirement already satisfied: pytz>=2018.4 in /usr/lib/python3/dist-packages (from cfg_load) (2021.1)\r\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/lib/python3/dist-packages (from cfg_load) (2.25.1)\r\n",
      "Requirement already satisfied: mpu[io]>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from cfg_load) (0.23.1)\r\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from cfg_load) (1.16.0)\r\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.9/dist-packages (from mpu[io]>=0.15.0->cfg_load) (4.1)\r\n",
      "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal->mpu[io]>=0.15.0->cfg_load) (0.1.0.post0)\r\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal->mpu[io]>=0.15.0->cfg_load) (2021.5)\r\n"
     ]
    }
   ],
   "source": [
    "%pip install cfg_load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44dd52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mininet.net import Mininet\n",
    "from mininet.net import Mininet,CLI\n",
    "from mininet.node import OVSKernelSwitch, Host\n",
    "from mininet.link import TCLink,Link\n",
    "from mininet.log import setLogLevel, info\n",
    "\n",
    "import time, os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MininetBackEnd(object):\n",
    "\n",
    "    def init_params(self, mu, sigma, link_bw, sla_bw):\n",
    "        self.mu = float(mu)\n",
    "        self.sigma = float(sigma)\n",
    "        self.sla_bw = float(sla_bw)\n",
    "        self.link_lat = float(0.0)\n",
    "        self.link_bw = float(link_bw)\n",
    "     \n",
    "    def reset_links(self):\n",
    "        self.current_link_failure = False\n",
    "        self.previous_link_failure = False\n",
    "\n",
    "        self.active_link = 0 # internet by default\n",
    "        self.episode_over = False\n",
    "\n",
    "        self.take_measurements()\n",
    "\n",
    "    def __init__(self, mu, sigma, link_bw, sla_bw, seed):\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.init_params(mu, sigma, link_bw, sla_bw)\n",
    "\n",
    "        self.net = Mininet( topo=None, listenPort=6633, ipBase='10.0.0.0/8')\n",
    "\n",
    "        self.h1 = self.net.addHost( 'host1', mac = '00:00:00:00:00:01', ip='10.0.0.1' )\n",
    "        self.h2 = self.net.addHost( 'host2', mac = '00:00:00:00:00:02', ip='10.0.0.2' )\n",
    "        self.h3 = self.net.addHost( 'noise1', mac = '00:00:00:00:00:03', ip='10.0.0.3' )\n",
    "        self.h4 = self.net.addHost( 'noise4', mac = '00:00:00:00:00:04', ip='10.0.0.4' )\n",
    "        self.s1 = self.net.addSwitch( 'edge1', cls=OVSKernelSwitch, protocols='OpenFlow13' )\n",
    "        self.s2 = self.net.addSwitch( 'edge2', cls=OVSKernelSwitch, protocols='OpenFlow13' )\n",
    "        self.s3 = self.net.addSwitch( 'core1', cls=OVSKernelSwitch, protocols='OpenFlow13' )\n",
    "        self.s4 = self.net.addSwitch( 'core2', cls=OVSKernelSwitch, protocols='OpenFlow13' )\n",
    "        self.net.addLink( self.h1, self.s1, cls=Link)\n",
    "        self.net.addLink( self.h2, self.s2, cls=Link)\n",
    "        self.net.addLink( self.h3, self.s1, cls=Link)\n",
    "        self.net.addLink( self.h4, self.s2, cls=Link)\n",
    "        self.net.addLink( self.s1, self.s3, cls=TCLink, bw=self.link_bw)\n",
    "        self.net.addLink( self.s1, self.s4, cls=TCLink, bw=self.link_bw)\n",
    "        self.net.addLink( self.s2, self.s3, cls=TCLink, bw=self.link_bw)\n",
    "        self.net.addLink( self.s2, self.s4, cls=TCLink, bw=self.link_bw)\n",
    "\n",
    "        self.net.start()\n",
    "\n",
    "\n",
    "        # add flows\n",
    "\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=20,ip,nw_dst=10.0.0.2,actions=output:4')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,ip,nw_dst=10.0.0.1,actions=output:1')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,ip,nw_dst=10.0.0.3,actions=output:2')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,arp,nw_dst=10.0.0.1,actions=output:1')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,arp,nw_dst=10.0.0.3,actions=output:2')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,arp,nw_dst=10.0.0.2,actions=normal')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,arp,nw_dst=10.0.0.4,actions=normal')\n",
    "        self.s1.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=10,ip,nw_dst=10.0.0.4,actions=output:4')\n",
    "\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=20,ip,nw_dst=10.0.0.1,actions=output:4')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,ip,nw_dst=10.0.0.2,actions=output:1')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,ip,nw_dst=10.0.0.4,actions=output:2')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,arp,nw_dst=10.0.0.2,actions=output:1')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,arp,nw_dst=10.0.0.4,actions=output:2')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,arp,nw_dst=10.0.0.1,actions=normal')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,arp,nw_dst=10.0.0.3,actions=normal')\n",
    "        self.s2.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  edge2 priority=10,ip,nw_dst=10.0.0.3,actions=output:4')\n",
    "\n",
    "\n",
    "        self.s3.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  core1 priority=10,in_port=1,actions=output:2')\n",
    "        self.s3.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  core1 priority=10,in_port=2,actions=output:1')\n",
    "        \n",
    "        self.s4.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  core2 priority=10,in_port=1,actions=output:2')\n",
    "        self.s4.cmd('ovs-ofctl --protocols=OpenFlow13 add-flow  core2 priority=10,in_port=2,actions=output:1')\n",
    "\n",
    "        #CLI(self.net)\n",
    "\n",
    "        # start udp  traffic receiver - simulate other traffic\n",
    "        self.h4.cmd(\"iperf  -u -s -i 1 >& /tmp/udp_server.log &\")\n",
    "\n",
    "        # start host tcp traffic receiver - simulate main flow\n",
    "        self.h2.cmd(\"iperf  -s -i 1  >& /tmp/tcp_server.log &\")\n",
    "\n",
    "        self.reset_links()\n",
    "\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.net.stop()\n",
    "\n",
    "\n",
    "    def take_measurements(self):\n",
    "        \"\"\" Send udp traffic and then take bandwidth measurement \"\"\"\n",
    "        # send udp traffic  - simulate other flow\n",
    "      \n",
    "        #udp_bw = np.random.normal(self.mu, self.sigma)\n",
    "        # cleanup /tmp for logs\n",
    "\n",
    "        os.system(\"rm /tmp/*.log\")\n",
    "\n",
    "        # send udp traffic  - simulate other flow\n",
    "        ip = self.h4.IP()\n",
    "        bw = np.random.normal(self.mu, self.sigma) \n",
    "        cmd = \"iperf -u -c {0} -b  {1}M -t 10  >& /tmp/udp_client.log &\".format(ip, bw)\n",
    "        cmd2 = \"ping {0} -c 5 >& /tmp/udp_client_lat.log &\".format(ip)\n",
    "        #info(cmd)\n",
    "        self.h3.cmd(cmd)\n",
    "        self.h3.cmd(cmd2)\n",
    "\n",
    "\n",
    "        ## we measure  internet link only, MPLS link => full BW \n",
    "        if self.active_link == 0:   \n",
    "            # send tcp  traffic  - main flow\n",
    "            ip = self.h2.IP()\n",
    "            cmd = \"iperf -c {0} -t 5 >& /tmp/tcp_client.log &\".format(ip)\n",
    "            cmd2 = \"ping {0} -c 5 >& /tmp/tcp_client_lat.log &\".format(ip)\n",
    "            #info(cmd)\n",
    "            self.h1.cmd(cmd)\n",
    "            self.h1.cmd(cmd2)\n",
    "\n",
    "        # wait for  traffic flow to settle\n",
    "        # if you set this too low, output file will not be generated properly !!!!!!\n",
    "        time.sleep(15)\n",
    "        \n",
    "        # always measure internet link available bw\n",
    "        self.available_bw = float(self.link_bw) - float(self.read_udp_bw())\n",
    "\n",
    "        if self.available_bw < 0.0:\n",
    "            self.available_bw = 0.0\n",
    "\n",
    "        ## we measure  internet link only, MPLS link => full BW \n",
    "        if self.active_link == 0:   \n",
    "            self.current_bw = np.random.normal(self.available_bw, 3)\n",
    "            self.link_lat = self.read_tcp_lat()\n",
    "        else:\n",
    "            self.current_bw = self.link_bw\n",
    "            self.link_lat = 5.0\n",
    "            \n",
    "    def read_tcp_bw(self):\n",
    "        bw = ['None']\n",
    "        with open('/tmp/tcp_client.log') as f:\n",
    "            for line in f:\n",
    "                #print ('line = ', line)\n",
    "                if 'bits/sec' in line:\n",
    "                    line = line.replace('-', ' ')\n",
    "                    fields = line.strip().split()\n",
    "                    # Array indices start at 0 unlike AWK\n",
    "\n",
    "                    if len(fields) > 7:\n",
    "                        bw.append(fields[7])\n",
    "\n",
    "        #print(\"TCP measure:\",bw[-1])\n",
    "        return(bw[-1])\n",
    "\n",
    "\n",
    "    def read_udp_bw(self):\n",
    "        bw = ['None']\n",
    "        with open('/tmp/udp_client.log') as f:\n",
    "            for line in f:\n",
    "                #print ('line = ', line)\n",
    "                if 'bits/sec' in line:\n",
    "                    line = line.replace('-', ' ')\n",
    "                    fields = line.strip().split()\n",
    "                    # Array indices start at 0 unlike AWK\n",
    "    \n",
    "                    if len(fields) > 7:\n",
    "                        bw.append(fields[7])\n",
    "\n",
    "        #print(\"Udp measure:\",bw[-1])\n",
    "        return(bw[-1])     \n",
    "            \n",
    "    def read_tcp_lat(self):\n",
    "        with open('/tmp/tcp_client_lat.log') as f:\n",
    "            for line in f:\n",
    "                #print ('line = ', line)\n",
    "                if 'rtt' in line:\n",
    "                    line = line.replace('-', ' ')\n",
    "                    fields = line.strip().split()\n",
    "                    #print(line)\n",
    "                    #print(fields[3])\n",
    "                    fields = fields[3].strip().split('/')\n",
    "                    #print(\"Latency: \",fields[1])\n",
    "                    return(fields[1])\n",
    "\n",
    "    def switch_flows(self, action):\n",
    "        if action == 0:\n",
    "            channel = 4\n",
    "        elif action == 1:\n",
    "            channel = 3\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        cmd = \"ovs-ofctl --protocols=OpenFlow13 add-flow  edge1 priority=20,ip,\\\n",
    "                    nw_dst=10.0.0.2,actions=output:{0}\".format(channel)\n",
    "        #info(cmd)\n",
    "        self.s1.cmd(cmd)\n",
    "\n",
    "\n",
    "\n",
    "    def switch_link(self, action):\n",
    "\n",
    "        # if action specifies same link as before it is not a switch\n",
    "        if action != self.active_link:\n",
    "            self.switch_flows(action)\n",
    "            \n",
    "\n",
    "        ## action is 0 => link is internet\n",
    "        ## action 1 => Fast link\n",
    "        self.active_link  = action\n",
    "\n",
    "        self.take_measurements()\n",
    "\n",
    "        ## Here is the logic that checks  two subsequent SLA failures\n",
    "        self.current_link_failure = False\n",
    "\n",
    "        # if current bandwidth less than SLA it is a failure\n",
    "        if self.active_link == 0:\n",
    "            if float(self.current_bw) < float(self.sla_bw):\n",
    "                info ('current link failure')\n",
    "                self.current_link_failure = True\n",
    "\n",
    "                # if it failed in previous tick also, mark it a link failure\n",
    "                if  self.previous_link_failure == True:\n",
    "                    info ('previous link also failure, episode over')\n",
    "                    self.episode_over = True\n",
    "            \n",
    "        # copy current to previous\n",
    "        self.previous_link_failure = self.current_link_failure \n",
    "        \n",
    "        return self.episode_over \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d8c71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging.config\n",
    "import math\n",
    "import pkg_resources\n",
    "import random\n",
    "\n",
    "# 3rd party modules\n",
    "from gym import spaces\n",
    "import cfg_load\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SdwanEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Define Sdwan environment.\n",
    "    The environment defines  how links will be selected based on bandwidth\n",
    "    availability \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_ticks=300):\n",
    "\n",
    "        # General variables defining the environment\n",
    "\n",
    "        self.LINK_BW = 10.0\n",
    "        self.LINK_SELECT_ACTION_INTERNET = 0\n",
    "        self.LINK_SELECT_ACTION_MPLS = 1\n",
    "        self.MAX_TICKS = max_ticks\n",
    "\n",
    "        self.backend = MininetBackEnd(mu=4, sigma=2, link_bw=self.LINK_BW, sla_bw=6, seed=100)\n",
    "\n",
    "        # Define what the agent can do\n",
    "        # Choose link1 or Link2 \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # Observation \n",
    "\n",
    "        low = np.array([self.LINK_SELECT_ACTION_INTERNET,  # active link\n",
    "                        0.0,  #current_bw\n",
    "                        0.0,  #available bw\n",
    "                        ])\n",
    "        high = np.array([self.LINK_SELECT_ACTION_MPLS, self.LINK_BW, self.LINK_BW])\n",
    "\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # episode over \n",
    "        self.episode_over = False\n",
    "        self.info = {} \n",
    "\n",
    "        # Store what the agent tried\n",
    "        self.curr_episode = -1\n",
    "        self.action_episode_memory = []\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        The agent takes a step in the environment.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "        Returns\n",
    "        -------\n",
    "        ob, reward, episode_over, info : tuple\n",
    "            ob (object) :\n",
    "                an environment-specific object representing your observation of\n",
    "                the environment.\n",
    "            reward (float) :\n",
    "                amount of reward achieved by the previous action. The scale\n",
    "                varies between environments, but the goal is always to increase\n",
    "                your total reward.\n",
    "            episode_over (bool) :\n",
    "                whether it's time to reset the environment again. Most (but not\n",
    "                all) tasks are divided up into well-defined episodes, and done\n",
    "                being True indicates the episode has terminated. (For example,\n",
    "                perhaps the pole tipped too far, or you lost your last life.)\n",
    "            info (dict) :\n",
    "                 diagnostic information useful for debugging. It can sometimes\n",
    "                 be useful for learning (for example, it might contain the raw\n",
    "                 probabilities behind the environment's last state change).\n",
    "                 However, official evaluations of your agent are not allowed to\n",
    "                 use this for learning.\n",
    "        \"\"\"\n",
    "        self.take_action(action)\n",
    "        reward = self.get_reward()\n",
    "        ob = self.get_state()\n",
    "        return ob, reward, self.episode_over, self.info \n",
    "\n",
    "    def take_action(self, action):\n",
    "        self.episode_over = self.backend.switch_link(action)\n",
    "                \n",
    "        self.ticks += 1\n",
    "\n",
    "        # check if episode ended by ERROR, then mark it in 'info'\n",
    "        if self.episode_over:\n",
    "            logging.info ('Episode ended by ERROR')\n",
    "            self.info['exit_status'] = 'ERROR'\n",
    "\n",
    "        # else Stop if max ticks over\n",
    "        elif self.ticks == self.MAX_TICKS:\n",
    "            logging.info ('Max ticks over, ending episode')\n",
    "            self.episode_over = True\n",
    "            self.info['exit_status'] = 'NORMAL'\n",
    "\n",
    "    def get_reward(self):\n",
    "\n",
    "        logging.debug('current bw:{0}, sla bw:{1}'.format(self.backend.current_bw, self.backend.sla_bw))\n",
    "\n",
    "        # maximum penalty for loosing the episode by ERROR\n",
    "        if self.episode_over and self.info['exit_status'] == 'ERROR':\n",
    "            return -5\n",
    "\t\n",
    "        # otherwise, reward for surviving this 'tick'\n",
    "        reward = 1\n",
    "\n",
    "        # every time we use the MPLS link reward is deducted\n",
    "        if self.backend.active_link == 1:\n",
    "            reward -= 3\n",
    "\n",
    "        # check bandwidth for internet link - if less than SLA then penalize\n",
    "        elif float(self.backend.current_bw)  <   float(self.backend.sla_bw):\n",
    "            logging.debug('BW is less than SLA')\n",
    "            reward -= 2\n",
    "\n",
    "        # everything fine - reward up\n",
    "        else:\n",
    "           reward += 2\n",
    "        \n",
    "        reward = reward - (float(self.backend.link_lat))/10.0\n",
    "        print('Bw:{0}, Latency {1}, Chosen action:{2}, Action Reward:{3}'.format(self.backend.current_bw, self.backend.link_lat, self.backend.active_link, reward))\n",
    "        \n",
    "        return reward\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        Returns\n",
    "        -------\n",
    "        observation (object): the initial observation of the space.\n",
    "        \"\"\"\n",
    "        self.curr_episode += 1\n",
    "        self.ticks = 0\n",
    "        self.action_episode_memory.append([])\n",
    "        self.backend.reset_links()\n",
    "        return self.get_state()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        return\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Get the observation.  it is a tuple \"\"\"\n",
    "        ob = (self.backend.active_link, self.backend.current_bw,  self.backend.available_bw)\n",
    "        return ob\n",
    "\n",
    "    def seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed\n",
    "\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.backend.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5b83b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SdwanEnv()\n",
    "n_actions = env.action_space.n\n",
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d427ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "#env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = np.finfo(np.float32).eps.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7256ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.63.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1380b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "  \"\"\"Combined actor-critic network.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      num_actions: int, \n",
    "      num_hidden_units: int):\n",
    "    \"\"\"Initialize.\"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "    self.actor = layers.Dense(num_actions)\n",
    "    self.critic = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    x = self.common(inputs)\n",
    "    return self.actor(x), self.critic(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5e0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = n_actions  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf55226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
    "\n",
    "  state, reward, done, _ = env.step(action)\n",
    "  d=np.array(state)\n",
    "  return (d.astype(np.float32), \n",
    "          np.array(reward, np.int32), \n",
    "          np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
    "  return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.int32, tf.int32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15f985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
    "\n",
    "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "  initial_state_shape = initial_state.shape\n",
    "  state = initial_state\n",
    "\n",
    "  for t in tf.range(max_steps):\n",
    "    # Convert state into a batched tensor (batch size = 1)\n",
    "    state = tf.expand_dims(state, 0)\n",
    "\n",
    "    # Run the model and to get action probabilities and critic value\n",
    "    action_logits_t, value = model(state)\n",
    "\n",
    "    # Sample next action from the action probability distribution\n",
    "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "    # Store critic values\n",
    "    values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "    # Store log probability of the action chosen\n",
    "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "    # Apply action to the environment to get next state and reward\n",
    "    state, reward, done = tf_env_step(action)\n",
    "    state.set_shape(initial_state_shape)\n",
    "\n",
    "    # Store reward\n",
    "    rewards = rewards.write(t, reward)\n",
    "\n",
    "    if tf.cast(done, tf.bool):\n",
    "      break\n",
    "\n",
    "  action_probs = action_probs.stack()\n",
    "  values = values.stack()\n",
    "  rewards = rewards.stack()\n",
    "\n",
    "  return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c41df23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80bce0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f4d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate expected returns\n",
    "    returns = get_expected_return(rewards, gamma)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, returns = [\n",
    "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "    # Calculating loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "  return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda4ace3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_episodes_criterion = 1000\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 30\n",
    "\n",
    "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 30\n",
    "running_reward = 0\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "\n",
    "    for i in t:\n",
    "        z=env.reset()\n",
    "        zz=np.array(z)\n",
    "        initial_state = tf.constant(zz, dtype=tf.float32)\n",
    "        episode_reward = int(train_step(\n",
    "            initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "        episodes_reward.append(episode_reward)\n",
    "        running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(\n",
    "        episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        # Show average episode reward every 10 episodes\n",
    "        if i % 1 == 0:\n",
    "            print(f'Episode {i}: average reward: {episode_reward}')\n",
    "\n",
    "        if i >= min_episodes_criterion:  \n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565829b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "182d9c2d627c7b4790f3f9c9bfc70f980bc2cf10c13b9e7f30579c1d1021dc25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
